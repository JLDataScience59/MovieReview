{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21855,
     "status": "ok",
     "timestamp": 1752667683010,
     "user": {
      "displayName": "Jérémy LESOT",
      "userId": "07743330766330191324"
     },
     "user_tz": -120
    },
    "id": "knslGh47OUb5",
    "outputId": "e02eb06b-fd4b-4ca2-96e4-199b84ed821e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 3166,
     "status": "ok",
     "timestamp": 1752667702475,
     "user": {
      "displayName": "Jérémy LESOT",
      "userId": "07743330766330191324"
     },
     "user_tz": -120
    },
    "id": "dDma4xV-Oq_F",
    "outputId": "88e36d3d-0450-4cbc-c90e-77e322877242"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/MovieReview.csv\")\n",
    "display(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "df = df.drop('sentiment', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 62207,
     "status": "ok",
     "timestamp": 1752667767798,
     "user": {
      "displayName": "Jérémy LESOT",
      "userId": "07743330766330191324"
     },
     "user_tz": -120
    },
    "id": "9YdnBlE7O1-U",
    "outputId": "2fbed459-f4d7-4817-b73f-48e4df76aea5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger uniquement les ressources nécessaires\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab') # Download the missing resource - removed this line\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = re.sub(r'\\b\\w{0,2}\\b', '', w)\n",
    "\n",
    "    mots = word_tokenize(w.strip())\n",
    "    mots = [mot for mot in mots if mot not in stop_words]\n",
    "    return ' '.join(mots).strip()\n",
    "\n",
    "# Supposons que df est ton DataFrame avec une colonne \"review\"\n",
    "df.review = df.review.apply(lambda x: preprocess_sentence(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5743,
     "status": "ok",
     "timestamp": 1752667795691,
     "user": {
      "displayName": "Jérémy LESOT",
      "userId": "07743330766330191324"
     },
     "user_tz": -120
    },
    "id": "5mBc7OuEQLiY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752667797092,
     "user": {
      "displayName": "Jérémy LESOT",
      "userId": "07743330766330191324"
     },
     "user_tz": -120
    },
    "id": "VjcwY8h3QXhX"
   },
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13433,
     "status": "ok",
     "timestamp": 1752667812819,
     "user": {
      "displayName": "Jérémy LESOT",
      "userId": "07743330766330191324"
     },
     "user_tz": -120
    },
    "id": "XNIcWAz5QpQJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sentenceToData(tokens, WINDOW_SIZE):\n",
    "    window = np.concatenate((np.arange(-WINDOW_SIZE,0),np.arange(1,WINDOW_SIZE+1)))\n",
    "    X,Y=([],[])\n",
    "    for word_index, word in enumerate(tokens) :\n",
    "        if ((word_index - WINDOW_SIZE >= 0) and (word_index + WINDOW_SIZE <= len(tokens) - 1)) :\n",
    "            X.append(word)\n",
    "            Y.append([tokens[word_index-i] for i in window])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "X, Y = ([], [])\n",
    "for review in df.review:\n",
    "    for sentence in review.split(\".\"):\n",
    "        word_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        if len(word_list) >= WINDOW_SIZE:\n",
    "            Y1, X1 = sentenceToData(word_list, WINDOW_SIZE//2)\n",
    "            X.extend(X1)\n",
    "            Y.extend(Y1)\n",
    "\n",
    "X = np.array(X).astype(int)\n",
    "y = np.array(Y).astype(int).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1752667821155,
     "user": {
      "displayName": "Jérémy LESOT",
      "userId": "07743330766330191324"
     },
     "user_tz": -120
    },
    "id": "oI8JbR-URAQD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tf0i3fDJRcHh",
    "outputId": "9215e6c0-1639-4718-ee97-3e1ccb548cd7"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size = 128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHCme7c3SQBu"
   },
   "outputs": [],
   "source": [
    "model.save(\"/content/drive/MyDrive/Movie_Review/word2vec.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTnBx2FyxNFW"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/content/drive/MyDrive/Movie_Review/tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM3RahK++RTigFk7/f5XMAF",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
